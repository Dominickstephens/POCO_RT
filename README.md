# REMI - Real-time Inference using POCO (with pocolib)

This section of the REMI project focuses on utilizing the POCO model (from the `pocolib` library) for real-time 3D human pose estimation. It includes scripts for running the standard PyTorch POCO model, a TensorRT-optimized version for accelerated inference, and utilities for converting models to TensorRT.

These POCO-based scripts serve as an alternative or comparative pose estimation backend within the broader REMI project. The metrics scripts are particularly valuable for evaluating the performance gains from TensorRT optimization and comparing POCO's real-time capabilities against other models like Ipman-R.

**Main REMI Project Page:** (https://dominickstephens.github.io/REMI_page/)
**Ipman-R (Alternative Pose Estimation) Component:** [https://github.com/Dominickstephens/ipman-r-RT](https://github.com/Dominickstephens/ipman-r-RT)
**Aitviewer (Real-time Visualization) Component:** [https://github.com/Dominickstephens/aitviewer-skel-RT](https://github.com/Dominickstephens/aitviewer-skel-RT)

## Overview

The scripts in this part of the project are based on the demonstration code provided with `pocolib`. They have been adapted and extended for the REMI project to:
* Run POCO inference on live webcam feeds.
* Integrate TensorRT for faster performance.
* Send estimated SMPL parameters over a network socket to a remote visualizer (like Aitviewer).
* Collect and log detailed performance metrics for both standard and TensorRT-optimized models.


``` bash
python demo4_trt.py --mode webcam --cfg configs/demo_poco_cliff.yaml --trt_model data/poco_model_trt.pt
```

This script is similar to demo2.py but is designed to run inference using a TensorRT-optimized POCO model (the .pt file generated by tensorrt_poco.py).

Model Loading: It loads the TorchScript model specified by --trt_model which contains the TensorRT engine.
Inference: The input batch for the TensorRT model is constructed as a tuple of tensors (matching the POCOTracerWrapper's forward signature: img, bbox_info, focal_length, scale, center, orig_shape). The model returns a tuple of output tensors, which are then mapped back to a dictionary (e.g., pred_pose, pred_shape).
The rest of the functionality (detection, socket communication, optional local display) is largely the same as demo2.py.
The primary benefit is expected to be higher inference speed (FPS) compared to demo2.py.


``` bash
python demo4_trt_metrics.py --mode webcam --cfg configs/demo_poco_cliff.yaml --trt_model data/poco_model_trt.pt
```
This script extends demo4_trt.py by adding the same comprehensive metrics collection capabilities found in demo2_metrics.py.

Functionality: It performs real-time inference with the TensorRT-optimized POCO model.
Metrics Collection, GUI, and CSV Logging: Identical to demo2_metrics.py, allowing for direct performance comparison between the standard and TensorRT-optimized models under various conditions. The CSV filename is typically pocolib_demo_metrics_avg.csv (it might be the same as demo2_metrics.py, so ensure you manage output files if running both).

``` bash
python demo2.py --mode webcam --cfg configs/demo_poco_cliff.yaml --ckpt data/poco_cliff.pt
```

This script runs real-time 3D human pose estimation using the standard PyTorch POCO model loaded via POCOTester.

Mode: Operates in webcam mode (can also support video, folder, directory from original pocolib demo).
Detection: Uses a multi-person tracker (e.g., YOLO via MPT from pocolib) to detect people in the webcam feed.
Inference: For each detected person, it prepares a batch and runs the POCO model to get pred_pose (rotation matrices or axis-angle), pred_shape (betas), and pred_cam (weak perspective camera parameters).
Socket Communication: If a relay server (like remote_smpl2.py from the Aitviewer component) is running on localhost:9999, this script will:
Convert the pose to axis-angle format.
Apply a 180-degree X-axis rotation to the global orientation.
Send the SMPL parameters (pose, shape, translation) to the relay server.
Local Display (Optional): If --display is used, it renders the estimated SMPL mesh overlaid on the webcam feed using pocolib.utils.vibe_renderer.Renderer.


``` bash
python demo2_metrics.py --mode webcam --cfg configs/demo_poco_cliff.yaml --ckpt data/poco_cliff.pt
```

This script extends demo2.py by adding comprehensive performance and stability metrics collection.

Functionality: It performs real-time inference with the standard PyTorch POCO model, including detection, socket communication, and optional local display, just like demo2.py.
Metrics Collection:
Uses a separate thread (metrics_calculation_worker) to calculate metrics based on the model's output for the first detected person.
Calculated metrics include: Processing FPS, End-to-End Latency, Pose Change (frame-to-frame), Translation Change, Joint Position Change, Shape Parameter Variance, and Detection Rate.
Buffers these metrics over a 5-second window.
GUI Interface: Provides a Tkinter GUI to:
Select the current "Condition" (e.g., Optimal, Low Light).
Trigger a 5-second recording of the averaged metrics.
CSV Logging: Saves the averaged metrics to a CSV file (e.g., pocolib_demo_no_trt_metrics_avg.csv).
Pressing 'r' in the OpenCV display window (if active) also triggers metric recording.

``` bash
python tensorrt_poco.py
```

This script is a utility for converting a standard PyTorch POCO model checkpoint (e.g., poco_cliff.pt) into a TensorRT-optimized TorchScript model (.pt file).

It loads the POCO model using POCOTester.
A POCOTracerWrapper is used to define a new forward signature that accepts individual tensors as input (required for TensorRT tracing with dynamic shapes on specific inputs like the image).
It traces the wrapped model and then compiles it using torch_tensorrt.compile with specified input shapes (dynamic batch size for image, static for others) and precision (FP32 or FP16).
The optimized model is saved to the specified output path (e.g., data/poco_model_trt.pt).
Note: You'll need to provide the correct paths to your POCO configuration file (--cfg) and the PyTorch checkpoint (--ckpt).



# For Installation, see below


# POCO: Pose and Shape Estimation with Confidence [3DV 2024]

[Sai Kumar Dwivedi](https://ps.is.mpg.de/person/sdwivedi), [Cordelia Schmid](https://thoth.inrialpes.fr/~schmid/), [Hongwei Yi](https://ps.is.mpg.de/person/hyi), [Michael J. Black](https://ps.is.mpg.de/person/black), [Dimitrios Tzionas](https://dtzionas.com)

**International Conference on 3D Vision (3DV 2024)**

[![Website shields.io](https://img.shields.io/website?url=http%3A//poco.is.tue.mpg.de)](https://poco.is.tue.mpg.de) [![YouTube Badge](https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube)](https://www.youtube.com/watch?v=rrAl90dYvZE)  [![arXiv](https://img.shields.io/badge/arXiv-2308.12965-00ff00.svg)](https://arxiv.org/abs/2308.12965)  


<div style="display:flex;">
    <img src="assets/run_lola.gif" width="45%" style="margin-right: 1%;">
    <img src="assets/yt_solo.gif" width="45%">
</div>


## Setup and Installation

Clone the repository: 
```shell
git clone https://github.com/saidwivedi/POCO.git
```

Create fresh conda environment and install all the dependencies:
```
conda create -n poco python=3.8
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge
pip install -r requirements.txt
```

## Pretrained Models

To run the demo, please download the pretrained models and necessary files from [here](https://poco.is.tue.mpg.de/download.php). Note that the Downloads url works only after sign in -- you need to register first and agree with our license.

After downloading, unzip the file and ensure that the contents are placed in a folder named `./data`.

## Demo

We provide two versions of POCO: POCO-PARE and POCO-CLIFF. Note that POCO-CLIFF is more suitable for in-the-wild scenarios. To run a specific model, make sure to change the `--cfg` and `--ckpt` parameters accordingly i.e `pare` for POCO-PARE and `cliff` for POCO-CLIFF.

### Run the demo on a video

```
python demo.py --mode video --vid_file demo_data/friends.mp4 --cfg configs/demo_poco_cliff.yaml --ckpt data/poco_cliff.pt --output_folder out
```

### Run the demo on image folder

```
python demo.py --mode folder --image_folder demo_data/images --cfg configs/demo_poco_cliff.yaml --ckpt data/poco_cliff.pt --output_folder out
```

## Acknowledgements

Parts of the code are taken or adapted from the following repos:
- [PARE](https://github.com/mkocabas/PARE)
- [CLIFF](https://github.com/huawei-noah/noah-research/tree/master/CLIFF)
- [RLE](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
- [BEDLAM](https://github.com/pixelite1201/BEDLAM)
- [RealNVP](https://github.com/senya-ashukha/real-nvp-pytorch/)

We thank Partha Ghosh and Haiwen Feng for insightful discussions, Priyanka Patel for the CLIFF implementation, and Peter Kulits, Shashank Tripathi, Muhammed Kocabas, and the Perceiving Systems department for their feedback. SKD acknowledges support from the International Max Planck Research School for Intelligent Systems (IMPRS-IS). This work was partially supported by the German Federal Ministry of Education and Research (BMBF): Tubingen AI Center, FKZ: 01IS18039B.

## Citing
If you find this code useful for your research, please consider citing the following paper:

```bibtex
@inproceedings{dwivedi_3dv2023_poco,
    title={{POCO}: {3D} Pose and Shape Estimation using Confidence},
    author={Dwivedi, Sai Kumar and Schmid, Cordelia and Yi, Hongwei and Black, Michael J. and Tzionas, Dimitrios},
    booktitle={International Conference on 3D Vision (3DV)},
    year={2024},
}
```

## License

This code is available for **non-commercial scientific research purposes** as defined in the [LICENSE file](LICENSE). By downloading and using this code you agree to the terms in the [LICENSE](LICENSE). Third-party datasets and software are subject to their respective licenses.

## Contact

For code related questions, please contact sai.dwivedi@tuebingen.mpg.de

For commercial licensing (and all related questions for business applications), please contact ps-licensing@tue.mpg.de.
